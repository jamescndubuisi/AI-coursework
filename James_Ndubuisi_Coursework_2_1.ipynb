{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uSLWMxnrrf7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# F29AI Coursework 2.1\n",
        "\n",
        "**Deadline:** on Friday, 30th Nov 2023\n",
        "\n",
        "**How to Submit:** To be submitted via Canvas â€“ Submission are timestamped: all submission after the deadline will be considered late\n",
        "\n",
        "\n",
        "## **Very important information (Do not miss any points)**:\n",
        "1. You can NOT edit this file, copy it to your own google collab account and edit it there.\n",
        "2. Your work have to be submitted in **Canvas** with a link to your google collab, otherwise your work won't be marked. Directly emailing to lecturer will NEVER work.\n",
        "3. Read carefully the instruction before each section, if you missed some tasks, you may lose marks. For example, if you are required to print some contents, please JUST print what asked.\n",
        "4. It is highly recoommended that you ONLY filled the functions or code blocks highlighted with the comments \"-- your code here --\", which is the easiest way to complete all tasks. However, as long as all the tasks are completed, you are fine.\n",
        "5. Your whole notebook will be tested using the \"Runtime->Run all\" option. No one will run each cell one after another. Make sure your notebook can be executed by one click. Only the results generated by the \"Runtime->Run all\" action.\n",
        "6. You MUST fill the following information, otherwise your work won't be marked.\n",
        "7. Only do one version of this coursework, if you submit a Java version too, we will ignore your Python version.\n",
        "8. The tasks you need to complete will be shown in <font color=\"blue\"> Blue </font>, with marks attached to it. Do not miss them.\n",
        "9. You are allowed, actually encouraged to use ChatGPT for the tasks of <font color=\"blue\"> Value Interation ONLY </font>, but allowed to use that too for QLearn. In this case, you must share the FULL conversation with ChatGPT by paste the link in the following form. One example is this [link](https://chat.openai.com/share/4f4c94ea-ca6a-47de-a4ae-5cc285b85d7d). If you use other LLMs, paste the screenshots of your conversation by inserting a new text cell below (before the \"task\" section).\n",
        "10. Do NOT generate the code using ChatGPT for policy iteration and q-learn. Once discovered, your marks for this two parts will be gone.\n",
        "11. This is an <font color=\"red\"> INDIVIDUAL </font> work, any submissions that are similar enough will be considered as IP case.\n",
        "\n",
        "<font color=\"red\"> Any questions about the above information, or about DEBUG process won't be answered by default!\n",
        "\n",
        "Remember: this python version of coursework won't have other official support.</font>\n",
        "\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "rz9VdFiNQy1v"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZbsSgv11rfVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your Information"
      ],
      "metadata": {
        "id": "J2o5CFdiGl2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Student Information\n",
        "First_name = \"James\" #@param {type:\"string\"}\n",
        "Family_name = \"Ndubuisi \" #@param {type:\"string\"}\n",
        "University_email = \"jn2033@hw.ac.uk\" #@param {type:\"string\"}\n",
        "Student_no = \"H00379625\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown - If you used ChatGPT for Value Iteration tasks share your conversation link below (optional).\n",
        "GPT_conversation_link = \"\" #@param {type:\"string\"}\n",
        "\n",
        "print(GPT_conversation_link)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-O6GBpaoHVYs",
        "outputId": "159b2433-126e-436f-9ba1-df89ef1b4a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C6Pu_B2_PSzj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "rp_WqKG0OGK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Tasks**\n",
        "* In this coursework, you will implement Value Iteration, Policy Iteration that plan/learn to play 3x3 Tic-Tac-Toe game. You will test your agents against other rule-based agents that are provided. You can also play against all the agents including your own agents to test them.\n",
        "* A general framework for the game and agents is provided. Run each code cell below in order, when you see a <>"
      ],
      "metadata": {
        "id": "bru7jT8VLX6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 0: Environment: packages, constants, basic code"
      ],
      "metadata": {
        "id": "iBek_YuBQzED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pickle\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Constants for the game\n",
        "EMPTY = 0\n",
        "PLAYER_X = 1\n",
        "PLAYER_O = -1\n",
        "GAME_ROW, GAME_COL = 3, 3\n"
      ],
      "metadata": {
        "id": "ZGtHyRUXVZwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 0.1: The ***Game*** class:\n",
        "\n",
        "play(): simulate one game\n",
        "\n",
        "*show_board* indicate whether the states are printed during play"
      ],
      "metadata": {
        "id": "_8tNVHrqQzTK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTW_W_wlQtve"
      },
      "outputs": [],
      "source": [
        "class Game:\n",
        "    \"\"\"\n",
        "    Define the tictactoe game. The function and variable names should be self explained.\n",
        "\n",
        "    @author: chenxy\n",
        "    \"\"\"\n",
        "    def __init__(self, player_x, player_o, show_board=False):\n",
        "        self.board = np.zeros((GAME_ROW, GAME_COL), dtype=int)\n",
        "        self.player_x = player_x\n",
        "        self.player_o = player_o\n",
        "        self.current_player = self.player_x\n",
        "        self.winner = None\n",
        "        self.show_board = show_board\n",
        "        self.turn = 0\n",
        "\n",
        "    def get_empty_positions(self):\n",
        "        return [(i, j) for i in range(GAME_ROW) for j in range(GAME_COL) if self.board[i, j] == EMPTY]\n",
        "\n",
        "    def is_winner(self, player):\n",
        "        symbol = player.symbol\n",
        "        for i in range(GAME_ROW):\n",
        "            if np.all(self.board[i, :] == symbol) or np.all(self.board[:, i] == symbol):\n",
        "                return True\n",
        "        if np.all(np.diag(self.board) == symbol) or np.all(np.diag(np.fliplr(self.board)) == symbol):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def is_draw(self):\n",
        "        return np.all(self.board != EMPTY)\n",
        "\n",
        "    def make_move(self, position):\n",
        "        if self.board[position] != EMPTY:\n",
        "            # Don't raise an exception, just return indicating an invalid move\n",
        "            return False\n",
        "        self.board[position] = self.current_player.symbol\n",
        "        return True\n",
        "\n",
        "    def switch_player(self):\n",
        "        self.current_player = self.player_x if self.current_player == self.player_o else self.player_o\n",
        "\n",
        "    def get_hash(self, board=None):\n",
        "        if board is None:\n",
        "            board = self.board\n",
        "        return ','.join(str(int(elem)) for elem in board.flatten())\n",
        "\n",
        "    def reset(self):\n",
        "        self.__init__(self.player_x, self.player_o, self.show_board)\n",
        "\n",
        "    def is_terminal(self):\n",
        "        # Check for a win in rows, columns, and diagonals\n",
        "        for i in range(GAME_ROW):\n",
        "            if np.all(self.board[i] == self.current_player.symbol) or \\\n",
        "               np.all(self.board[:, i] == self.current_player.symbol):\n",
        "                return True\n",
        "        if np.all(np.diag(self.board) == self.current_player.symbol) or \\\n",
        "           np.all(np.diag(np.fliplr(self.board)) == self.current_player.symbol):\n",
        "            return True\n",
        "        # Check for a draw (no empty positions left)\n",
        "        if not np.any(self.board == EMPTY):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def play(self):\n",
        "        self.reset()\n",
        "        while True:\n",
        "          position = self.current_player.move(self)\n",
        "          if not self.make_move(position):  # If move is invalid, skip to next turn\n",
        "            raise Exception(\"Something is wrong! No empty positions now!\")\n",
        "          self.turn+=1\n",
        "\n",
        "          if self.is_terminal():\n",
        "              if self.is_winner(self.current_player):\n",
        "                  self.winner = self.current_player.symbol\n",
        "                  print(f\"Player {self.current_player.symbol} wins!\")\n",
        "                  break  # Exit the loop immediately after a win\n",
        "\n",
        "              if self.is_draw():\n",
        "                  print(\"It's a draw!\")\n",
        "                  break  # Exit the loop immediately after a draw\n",
        "\n",
        "          if self.show_board:\n",
        "              print(f\"Turn {self.turn}: Player {self.current_player.symbol}\")\n",
        "              self.print_board()\n",
        "\n",
        "          self.switch_player()\n",
        "\n",
        "        if self.show_board:\n",
        "            self.print_board()  # Show the final board state\n",
        "\n",
        "    def print_board(self):\n",
        "        symbols = {EMPTY: ' ', PLAYER_X: 'X', PLAYER_O: 'O'}\n",
        "        for i in range(GAME_ROW):\n",
        "            print('|' + '|'.join(symbols[s] for s in self.board[i]) + '|')\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 0.2 Agent abstract class, all *agents* class inherit this one.\n",
        "* *RandomAgent*: perform random action\n",
        "* *AggressiveAgent*: choose the winning action\n",
        "* *DefensiveAgent*: stop opponent's winning action"
      ],
      "metadata": {
        "id": "-t2uA-1SRPx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent(ABC):\n",
        "    def __init__(self, symbol):\n",
        "        self.symbol = symbol\n",
        "        self.states_value = {}  # State values used by ValueIterationAgent\n",
        "\n",
        "    @abstractmethod\n",
        "    def move(self, game):\n",
        "        pass\n",
        "\n",
        "    def save_policy(self, file_name):\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(self.states_value, f)\n",
        "\n",
        "    def load_policy(self, file_name):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            self.states_value = pickle.load(f)\n",
        "\n",
        "class RandomAgent(Agent):\n",
        "    def move(self, game):\n",
        "        empty_cells = game.get_empty_positions()\n",
        "        if not empty_cells:\n",
        "            raise ValueError(\"No more moves left to play.\")\n",
        "        # Select a random move from the list of empty cells\n",
        "        return empty_cells[np.random.randint(len(empty_cells))]\n",
        "\n",
        "class AggressiveAgent(Agent):\n",
        "    def __init__(self, symbol):\n",
        "        super().__init__(symbol)\n",
        "\n",
        "    def move(self, game):\n",
        "        empty_positions = game.get_empty_positions()\n",
        "        board_copy = game.board.copy()\n",
        "        for position in empty_positions:\n",
        "            board_copy[position] = self.symbol\n",
        "            if game.is_winner(self):\n",
        "                return position\n",
        "            board_copy[position] = EMPTY  # Reset the position after check\n",
        "\n",
        "        # If no winning move found, return a random move\n",
        "        return empty_positions[np.random.choice(len(empty_positions))]\n",
        "\n",
        "class DefensiveAgent(Agent):\n",
        "    def __init__(self, symbol):\n",
        "        super().__init__(symbol)\n",
        "\n",
        "    def move(self, game):\n",
        "        opponent_symbol = PLAYER_O if self.symbol == PLAYER_X else PLAYER_X\n",
        "        empty_positions = game.get_empty_positions()\n",
        "        board_copy = game.board.copy()\n",
        "\n",
        "        # First, check if the opponent has a winning move and block it\n",
        "        for position in empty_positions:\n",
        "            board_copy[position] = opponent_symbol\n",
        "            if game.is_winner(self.__opponent()):\n",
        "                return position  # Block the opponent's winning move\n",
        "            board_copy[position] = EMPTY  # Reset the position after check\n",
        "\n",
        "        # If no blocking move is necessary, choose a random move\n",
        "        return empty_positions[np.random.choice(len(empty_positions))]\n",
        "\n",
        "    def __opponent(self):\n",
        "        # Private helper method to create a 'dummy' opponent with the opposite symbol\n",
        "        return RandomAgent(PLAYER_O if self.symbol == PLAYER_X else PLAYER_X)"
      ],
      "metadata": {
        "id": "IfdUxuKtRYqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 0.3: Useful and example functions:\n",
        "Some may never been called"
      ],
      "metadata": {
        "id": "ulB1esGpTBDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hash(board=None):\n",
        "    return ','.join(str(int(elem)) for elem in board.flatten())\n",
        "\n",
        "\n",
        "def get_hashes(boards):\n",
        "    return [get_hash(board) for board in boards]\n",
        "\n",
        "def valid_state(board, symbol):\n",
        "\n",
        "    # check the board state is valid\n",
        "    if symbol==PLAYER_X:\n",
        "        return (np.sum(board==PLAYER_X)==np.sum(board==PLAYER_O))\n",
        "    if symbol==PLAYER_O:\n",
        "        return (np.sum(board==PLAYER_X)-np.sum(board==PLAYER_O)==1)\n",
        "\n",
        "def next_symbol(board):\n",
        "    if (np.sum(board==PLAYER_X)==np.sum(board==PLAYER_O)):\n",
        "        return PLAYER_X\n",
        "    else:\n",
        "        return PLAYER_O\n",
        "\n",
        "def is_terminal(board):\n",
        "    # Check for a win in rows, columns, and diagonals\n",
        "    for i in range(GAME_ROW):\n",
        "        if abs(np.sum(board[i, :])) == 3 or abs(np.sum(board[:, i])) == 3:\n",
        "            return True\n",
        "    if abs(sum(np.diag(board))) == 3 or abs(sum(np.diag(np.fliplr(board)))) == 3:\n",
        "        return True\n",
        "    # Check for a draw\n",
        "    if not np.any(board == EMPTY):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_reward(board, symbol):\n",
        "    # Define opponent's symbol\n",
        "    opponent_symbol = PLAYER_O if symbol == PLAYER_X else PLAYER_X\n",
        "\n",
        "    # Check for current player's win\n",
        "    for i in range(GAME_ROW):\n",
        "        if sum(board[i, :]) == GAME_ROW * symbol or sum(board[:, i]) == GAME_COL * symbol:\n",
        "            return 1\n",
        "    if sum(np.diag(board)) == GAME_ROW * symbol or sum(np.diag(np.fliplr(board))) == GAME_COL * symbol:\n",
        "        return 1\n",
        "\n",
        "    # Check for opponent's win\n",
        "    for i in range(GAME_ROW):\n",
        "        if sum(board[i, :]) == GAME_ROW * opponent_symbol or sum(board[:, i]) == GAME_COL * opponent_symbol:\n",
        "            return -1\n",
        "    if sum(np.diag(board)) == GAME_ROW * opponent_symbol or sum(np.diag(np.fliplr(board))) == GAME_COL * opponent_symbol:\n",
        "        return -1\n",
        "\n",
        "    # Check for a draw\n",
        "    if is_terminal(board):\n",
        "        return 0\n",
        "\n",
        "    # For non-terminal states, the immediate reward is 0.\n",
        "    return 0\n",
        "\n",
        "\n",
        "def get_empty_positions(board):\n",
        "    return [(i, j) for i in range(GAME_ROW) for j in range(GAME_COL) if board[i, j] == EMPTY]\n",
        "\n",
        "def generate_next_boardstates(board, symbol):\n",
        "\n",
        "    # check the board state is valid\n",
        "    if symbol==PLAYER_X:\n",
        "        assert(np.sum(board==PLAYER_X)==np.sum(board==PLAYER_O))\n",
        "    if symbol==PLAYER_O:\n",
        "        assert(np.sum(board==PLAYER_X)-np.sum(board==PLAYER_O)==1)\n",
        "\n",
        "    # generate all next board states\n",
        "    all_empty_positions = get_empty_positions(board)\n",
        "    all_boards = np.tile(board, (len(all_empty_positions), 1, 1))\n",
        "    all_indices = np.concatenate(\n",
        "        [\n",
        "            np.expand_dims(np.arange(len(all_empty_positions)), axis=1),\n",
        "            np.array(all_empty_positions)\n",
        "        ], axis=1\n",
        "        )\n",
        "    all_boards[all_indices[:,0], all_indices[:,1], all_indices[:,2]] = symbol\n",
        "    all_boards = np.split(all_boards, all_boards.shape[0], axis=0)\n",
        "    return [np.squeeze(board) for board in all_boards]\n",
        "\n",
        "def generate_all_states(board, all_states=None, stop_step=None):\n",
        "\n",
        "    # assert(valid_state(board, symbol)) # validate the states and symbol\n",
        "\n",
        "    if all_states is None:\n",
        "        # all_states = {}\n",
        "        boards = [board]\n",
        "        state_hashes = [get_hash(board)]\n",
        "        p0 = 0\n",
        "        p1 = 1\n",
        "        p2 = p1\n",
        "        step = 0\n",
        "        step_symbol = next_symbol(board)\n",
        "\n",
        "    while p0!=p1:\n",
        "      for p_state in range(p0,p1):\n",
        "          # print(step)\n",
        "          # print('p_state:', p_state)\n",
        "          # print('board:', boards[p_state])\n",
        "          # print('step_symbol:', step_symbol)\n",
        "          # print(p1-p0)\n",
        "          # print('------------------------')\n",
        "          if is_terminal(boards[p_state]):\n",
        "              continue\n",
        "          next_boards = generate_next_boardstates(boards[p_state], step_symbol)\n",
        "          next_hashes = get_hashes(next_boards)\n",
        "\n",
        "          # print(next_boards)\n",
        "\n",
        "          boards+=next_boards\n",
        "          state_hashes+=next_hashes\n",
        "          p2 += len(next_boards)\n",
        "\n",
        "      step_symbol = PLAYER_X if step_symbol == PLAYER_O else PLAYER_O\n",
        "      p0 = p1\n",
        "      p1 = p2\n",
        "      step+=1\n",
        "\n",
        "      if stop_step is not None:\n",
        "        if step == stop_step:\n",
        "          break\n",
        "    return dict(zip(state_hashes, boards))"
      ],
      "metadata": {
        "id": "YMSydRNZTD2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 0.4: Fenerate all states using the function **generate_all_states** and save the states hush table in the local path for the future use."
      ],
      "metadata": {
        "id": "UtdFtIMCU9hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_hush_fname = \"all_states_hush1.txt\"\n",
        "if os.path.isfile(state_hush_fname):\n",
        "    with open(state_hush_fname, 'rb') as f:\n",
        "        all_states = pickle.load(f)\n",
        "\n",
        "else:\n",
        "  temp_board = np.zeros((GAME_ROW, GAME_COL))\n",
        "  all_states = generate_all_states(temp_board)\n",
        "\n",
        "  # same hush table of all the states\n",
        "  with open(state_hush_fname, 'wb') as f:\n",
        "      pickle.dump(all_states, f)\n",
        "\n",
        "print(\"In total, \", len(all_states), \"states\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yYr9IFtCVQOK",
        "outputId": "4e75a2f0-d295-4a53-afba-cb5173b9eeb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In total,  5478 states\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "XgZE3uSzeb5H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\"> **Task 1 (7 marks)**:</font> Value Iteration"
      ],
      "metadata": {
        "id": "4zpn3yEZXGFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\"> **Question 1:** </font> Write a value iteration agent in ValueIterationAgent which has been partially specified for you. Here you need to implement the train() & get_reward() methods. The former should perform **planning* using *value iteration and the latter should extract the policy and compute state values."
      ],
      "metadata": {
        "id": "N7lNh8ZYYn9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueIterationAgent(Agent):\n",
        "    def __init__(self, symbol, discount_factor=0.9, living_reward=-0.01):\n",
        "        super().__init__(symbol)\n",
        "        if 'all_states' in globals():\n",
        "            self.all_states = all_states\n",
        "        elif os.path.isfile(state_hush_fname):\n",
        "            with open(state_hush_fname, 'rb') as f:\n",
        "                self.all_states = pickle.load(f)\n",
        "        else:\n",
        "            raise Exception(\"No state hushes! Either run the code by order or create the state hush yourself\")\n",
        "\n",
        "        self.discount_factor = discount_factor  # Discount factor for future rewards\n",
        "        self.living_reward = living_reward  # Reward for living (negative for penalty)\n",
        "        self.value_function = {state: 0 for state in all_states.keys()}  # Initialize state values to 0\n",
        "\n",
        "        # self.all_states = all_states  # All possible states\n",
        "        self.win_reward=10.0;\n",
        "        self.lose_reward=-50.0;\n",
        "        self.living_reward=-1.00;\n",
        "        self.draw_reward=0.0;\n",
        "\n",
        "        self.policy = {}  # Initialize policy\n",
        "\n",
        "    def get_reward(self, board, symbol):\n",
        "        # Define opponent's symbol\n",
        "        opponent_symbol = PLAYER_O if symbol == PLAYER_X else PLAYER_X\n",
        "\n",
        "        # Check for current player's win\n",
        "        for i in range(GAME_ROW):\n",
        "            if sum(board[i, :]) == GAME_ROW * symbol or sum(board[:, i]) == GAME_COL * symbol:\n",
        "                return self.win_reward\n",
        "        if sum(np.diag(board)) == GAME_ROW * symbol or sum(np.diag(np.fliplr(board))) == GAME_COL * symbol:\n",
        "            return self.win_reward\n",
        "\n",
        "        # Check for opponent's win\n",
        "        # -- Your Code Here ---\n",
        "\n",
        "        # Check for a draw\n",
        "        # -- Your Code Here --\n",
        "\n",
        "        # For non-terminal states, the immediate reward is 0.\n",
        "        return 0\n",
        "\n",
        "    def train(self, threshold=0.00001):\n",
        "        # Value iteration algorithm\n",
        "        # -- Your Code Here--\n",
        "\n",
        "    def move(self, game):\n",
        "        # Return the move based on the current policy\n",
        "        current_state = game.get_hash()\n",
        "        if current_state in self.policy:\n",
        "            return self.policy[current_state]\n",
        "        else:\n",
        "            # In case current state is not in the policy, choose a random move\n",
        "            empty_positions = game.get_empty_positions()\n",
        "            return empty_positions[np.random.choice(len(empty_positions))]\n"
      ],
      "metadata": {
        "id": "DLYzW35pbLSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">Q1.1 (3/7): Run the following example of a Value Iteration \"X\" player against a Random \"O\" agent </font>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AjfOjBUnXr7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "player_x = ValueIterationAgent(PLAYER_X)  # This is the value iteration agent\n",
        "player_o = RandomAgent(PLAYER_O)  # This is the random agent\n",
        "\n",
        "game = Game(player_x, player_o)\n",
        "# print(game.board)\n",
        "\n",
        "# Compute the policy using value iteration only for the value iteration agent\n",
        "player_x.train()  # We only need to compute this for player O\n",
        "\n",
        "# Play the game\n",
        "\n",
        "game.play()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mW5HT2e-XySW",
        "outputId": "ff6268e2-19f2-4919-b2bd-aa51abfa80b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Player 1 wins!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\"> Q1.2 (3/7): Based on the example (Game 0: RandomAgent \"O\" v.s. ValueIterationAgent \"X\") above, run the following game: </font>\n",
        "* Game1: RandomAgent \"X\" v.s. ValueIterationAgent \"O\"\n",
        "* Game2: ValueIterationAgent \"X\" v.s. AggressiveAgent \"O\"\n",
        "* Game3: ValueIterationAgent \"O\" v.s. AggressiveAgent \"X\"\n",
        "* Game4: ValueIterationAgent \"X\" v.s. DefensiveAgent \"O\"\n",
        "* Game 5: ValueIterationAgent \"O\" v.s. DefensiveAgent \"X\"\n",
        "\n",
        "Use the one single code cell *below*"
      ],
      "metadata": {
        "id": "EXulxNJPgA2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Game 1:\n",
        "\n",
        "# Game 2:\n",
        "\n",
        "# Game 3:\n",
        "\n",
        "# Game 4:\n",
        "\n",
        "# Game 5:"
      ],
      "metadata": {
        "id": "_Cdq_YzynN10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\"> Q1.3 (1/7): Repeat the games (Game 0-5) above 50 rounds each Game. Using ValueIterationAgent, print out number of *wins*, *losts* and *draw* </font>"
      ],
      "metadata": {
        "id": "XCD_2WhQnOgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1.2 1-5:\n",
        "\n",
        "game.show_board = False # Disable printing board, don't change\n",
        "\n",
        "# -- Your Code Here ---"
      ],
      "metadata": {
        "id": "vX2Wr3twhT1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RfHFssjEhac1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "WHWrhsv2hZHp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\"> **Task 2** (7 marks):</font>  Policy Iteration\n",
        "\n",
        "Write a Policy Iteration agent in PolicyIterationAgent by implementing the policy_evaluation(), policy_improvement(), train() methods. The policy_evaluation() method should evaluate the current policy (see your lecture notes). The current values for the current policy should be stored in the provided policyValues map. The policy_improvement() method performs the Policy improvement step, and updates curPolicy. The train() method is the planning process, once done, an optimal policy should be saved in the agent object."
      ],
      "metadata": {
        "id": "YNeTi_PNjG0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "class PolicyIterationAgent(Agent):\n",
        "    def __init__(self, symbol, discount_factor=0.9, living_reward=-0.01):\n",
        "        super().__init__(symbol)\n",
        "\n",
        "        if 'all_states' in globals():\n",
        "            self.all_states = all_states\n",
        "        elif os.path.isfile(state_hush_fname):\n",
        "            with open(state_hush_fname, 'rb') as f:\n",
        "                self.all_states = pickle.load(f)\n",
        "        else:\n",
        "            raise Exception(\"No state hushes! Either run the code by order or create the state hush yourself\")\n",
        "\n",
        "        self.discount_factor = discount_factor  # Discount factor for future rewards\n",
        "        self.living_reward = living_reward  # Reward for living (negative for penalty)\n",
        "        self.value_function = {state: 0 for state in all_states.keys()}  # Initialize state values to 0\n",
        "\n",
        "        # self.all_states = all_states  # All possible states\n",
        "        self.win_reward=10.0;\n",
        "        self.lose_reward=-50.0;\n",
        "        self.living_reward=-1.00;\n",
        "        self.draw_reward=0.0;\n",
        "\n",
        "        self.all_states = all_states  # All possible states\n",
        "        self.discount_factor = discount_factor  # Discount factor for future rewards\n",
        "        # self.living_reward = living_reward  # Living reward (negative for penalty)\n",
        "        self.value_function = {state: 0 for state in all_states.keys()}  # Initialize state values to 0\n",
        "        # self.policy = {state: np.random.choice(get_empty_positions(board))\n",
        "        #                for state, board in all_states.items() if not is_terminal(board)}  # Random initial policy\n",
        "        # # Choosing a random tuple from the list of empty positions\n",
        "        self.policy = {state: random.choice(get_empty_positions(board))\n",
        "                   for state, board in all_states.items() if not is_terminal(board)}  # Random initial policy\n",
        "\n",
        "    def get_reward(self, board, symbol):\n",
        "        # -- Your Code Here ---\n",
        "        pass\n",
        "\n",
        "    def policy_evaluation(self, threshold=0.0001):\n",
        "        # -- Your Code Here ---\n",
        "        pass\n",
        "\n",
        "    def policy_improvement(self):\n",
        "        # -- Your Code Here --\n",
        "\n",
        "        pass\n",
        "\n",
        "    def train(self):\n",
        "        while True:\n",
        "            self.policy_evaluation()\n",
        "            if self.policy_improvement():\n",
        "                break\n",
        "\n",
        "    def move(self, game):\n",
        "        # Return the move based on the current policy\n",
        "        current_state = game.get_hash()\n",
        "        if current_state in self.policy:\n",
        "            return self.policy[current_state]\n",
        "        else:\n",
        "            # If the current state is not in the policy, choose a random move\n",
        "            empty_positions = game.get_empty_positions()\n",
        "            return empty_positions[np.random.choice(len(empty_positions))]\n"
      ],
      "metadata": {
        "id": "04vRz54IkLY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">Q2.1 (3/7): Run the following: Iteration \"X\" player against a Random \"O\" agent </font>"
      ],
      "metadata": {
        "id": "XUDOQc24l5Wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "player_o = RandomAgent(PLAYER_O)  # This is the random agent\n",
        "player_x = PolicyIterationAgent(PLAYER_X)  # This is the value iteration agent\n",
        "\n",
        "# player_x = ValueIterationAgent(PLAYER_X)  # This is the random agent\n",
        "# player_o = RandomAgent(PLAYER_O)  # This is the value iteration agent\n",
        "\n",
        "game = Game(player_x, player_o)\n",
        "# print(game.board)\n",
        "\n",
        "# Compute the policy using value iteration only for the value iteration agent\n",
        "player_x.train()  # We only need to compute this for player O\n",
        "\n",
        "# Play the game\n",
        "\n",
        "game.play()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKzFj62Ml4oK",
        "outputId": "1314c611-2f9c-4900-e4d1-803c026055ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Player 1 wins!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\"> Q2.2 (3/7): Based on the example (Game 0: RandomAgent \"O\" v.s. PlicyIterationAgent \"X\") above, run the following game: </font>\n",
        "* Game1: RandomAgent \"X\" v.s. PolicyIterationAgent \"O\"\n",
        "* Game2: PolicyIterationAgent \"X\" v.s. AggressiveAgent \"O\"\n",
        "* Game3: PolicyIterationAgent \"O\" v.s. AggressiveAgent \"X\"\n",
        "* Game4: PolicyIterationAgent \"X\" v.s. DefensiveAgent \"O\"\n",
        "* Game 5: PolicyIterationAgent \"O\" v.s. DefensiveAgent \"X\"\n",
        "\n",
        "Use the one single code cell *below*"
      ],
      "metadata": {
        "id": "Y3DVPAExmqVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Game 1:\n",
        "\n",
        "# Game 2:\n",
        "\n",
        "# Game 3:\n",
        "\n",
        "# Game 4:\n",
        "\n",
        "# Game 5:"
      ],
      "metadata": {
        "id": "L6mgJfvYnfMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\"> Q2.3 (1/7): Repeat the games (Game 0-5) above 50 rounds each Game. Using PolicyIterationAgent, print out number of *wins*, *losts* and *draw* </font>"
      ],
      "metadata": {
        "id": "crIFJl3_nlcx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "game.show_board = False\n",
        "\n",
        "# -- Your Code Here ---"
      ],
      "metadata": {
        "id": "HCyXqiiznLei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"blue\"> **Task 2** (6 marks):</font>  Q-Learn\n",
        "\n",
        "Write a QLearn agent in QLearnIterationAgent. No specific requirements of functions, but the planning process have to be done in a plan() function."
      ],
      "metadata": {
        "id": "4zuio47AnyyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QLearningAgent(Agent):\n",
        "\n",
        "    def __init__(self, symbol, alpha=0.4, gamma=0.9, epsilon=0.1, living_penalty=-1):\n",
        "        super().__init__(symbol)\n",
        "        self.alpha = alpha  # Learning rate\n",
        "        self.gamma = gamma  # Discount factor\n",
        "        self.epsilon = epsilon  # Epsilon for the epsilon-greedy policy\n",
        "        self.Q = {}  # Initialize Q-table\n",
        "        self.living_penalty = living_penalty\n",
        "        self.win_reward = 10.0\n",
        "        self.lose_reward = -50.0\n",
        "        self.draw_reward = 0.0\n",
        "        self.initial_state = np.zeros((GAME_ROW, GAME_COL), dtype=int)  # Initialize the initial state\n",
        "        self.current_symbol = symbol  # The symbol of the current player\n",
        "\n",
        "    # -- Your Code Here---\n",
        "\n",
        "\n",
        "    def train(self, num_episodes=100000):\n",
        "        # -- Your Code Here --\n",
        "        pass\n",
        "\n",
        "\n",
        "    def hash_state(self, state):\n",
        "        return str(state.reshape(GAME_ROW * GAME_COL))\n",
        "\n",
        "    def get_available_actions(self, state):\n",
        "        return [(i, j) for i in range(GAME_ROW) for j in range(GAME_COL) if state[i, j] == EMPTY]\n",
        "\n",
        "    def make_move(self, state, action, symbol):\n",
        "        new_state = np.array(state)\n",
        "        new_state[action] = symbol\n",
        "        reward = self.get_reward(new_state, symbol)\n",
        "        done = is_terminal(new_state)\n",
        "        return new_state, reward, done\n",
        "\n",
        "\n",
        "    def move(self, game):\n",
        "        # Extract the board from the Game object\n",
        "        board = game.board\n",
        "        state_hash = self.hash_state(board)\n",
        "        available_actions = self.get_available_actions(board)\n",
        "\n",
        "        if not available_actions:\n",
        "            raise ValueError(\"No available actions to make a move.\")\n",
        "\n",
        "        # Choose the best action based on the Q-table\n",
        "        action = self.choose_action(state_hash, available_actions)\n",
        "\n",
        "        # Convert action to the format expected by Game's make_move method (e.g., (row, col))\n",
        "        return action\n"
      ],
      "metadata": {
        "id": "XZkZXUbyy-UO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\">Q3.1 (3/6): Run the following example: Iteration \"X\" player against a Random \"O\" agent </font>"
      ],
      "metadata": {
        "id": "YiHdvxkMqosW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the QLearningAgent with its symbol (X or O)\n",
        "q_learning_agent = QLearningAgent(PLAYER_X)\n",
        "\n",
        "# Assume there is a random agent for the opponent\n",
        "random_agent = RandomAgent(PLAYER_O)\n",
        "\n",
        "# Initialize the game environment with both agents\n",
        "game = Game(q_learning_agent, random_agent)\n",
        "\n",
        "# Train the QLearningAgent with a function that simulates playing the game\n",
        "# The train function would need to be implemented to simulate games within the agent\n",
        "q_learning_agent.train()\n",
        "\n",
        "# Use the game's play function to start playing\n",
        "game.play()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCfNHR8joh8W",
        "outputId": "c81258ae-7072-48e0-fc06-67abf3169751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Player 1 wins!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\"> Q3.2 (2/6): Based on the example (Game 0: RandomAgent \"O\" v.s. QLearnAgent \"X\") above, run the following game: </font>\n",
        "* Game1: RandomAgent \"X\" v.s. QLearnAgent \"O\"\n",
        "* Game2: QLearnAgent \"X\" v.s. AggressiveAgent \"O\"\n",
        "* Game3: QLearnAgent \"O\" v.s. AggressiveAgent \"X\"\n",
        "* Game4: QLearnAgent \"X\" v.s. DefensiveAgent \"O\"\n",
        "* Game 5: QLearnAgent \"O\" v.s. DefensiveAgent \"X\"\n",
        "\n",
        "Use the one single code cell *below*"
      ],
      "metadata": {
        "id": "0nClnharq80A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Game 1:\n",
        "\n",
        "# Game 2:\n",
        "\n",
        "# Game 3:\n",
        "\n",
        "# Game 4:\n",
        "\n",
        "# Game 5:"
      ],
      "metadata": {
        "id": "I4WgO8SHrPgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <font color=\"blue\"> Q3.3 (1/7): Repeat the games (Game 0-5) above 50 rounds each Game. Using QLearnAgent, print out number of *wins*, *losts* and *draw* </font>"
      ],
      "metadata": {
        "id": "8ZAp7JyorMac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "game.show_board = False\n",
        "\n",
        "# -- Your Code Here --"
      ],
      "metadata": {
        "id": "wnNjSEt2xEo4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}